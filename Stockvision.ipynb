{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. Load the dataset"
      ],
      "metadata": {
        "id": "WKCdaherZnbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbGt4U_5lOK9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, SpatialDropout1D, Bidirectional, Dropout, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "file_path = '/content/2021-2024 final_New.excel'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Display the first few rows to understand the structure\n",
        "print(\"Dataset Preview:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. DataPreprocessing"
      ],
      "metadata": {
        "id": "Jd6jtGCeZ81H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where the Title or Prediction is missing\n",
        "df.dropna(subset=['Title', 'Decision'], inplace=True)\n",
        "\n",
        "# Convert the 'Date' column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Remove rows with invalid dates\n",
        "df.dropna(subset=['Date'], inplace=True)\n",
        "\n",
        "# Check for duplicates\n",
        "df.drop_duplicates(subset=['Title'], inplace=True)\n",
        "\n",
        "# Clean text by removing special characters, numbers, and stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove special characters and numbers\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "df['Cleaned_Title'] = df['Title'].apply(clean_text)\n",
        "\n",
        "# Display cleaned data\n",
        "print(\"\\nCleaned Data Sample:\")\n",
        "print(df[['Date', 'Cleaned_Title', 'Decision']].head())\n"
      ],
      "metadata": {
        "id": "dFeOOtORlZeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the cleaned text using TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf.fit_transform(df['Cleaned_Title']).toarray()\n",
        "\n",
        "# Encode the target variable ('Prediction')\n",
        "df['Label'] = df['Decision'].apply(lambda x: 1 if x == 'Positve' else 0)\n",
        "y = df['Label']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nData Split Complete\")\n",
        "print(f\"Training Samples: {len(X_train)}, Testing Samples: {len(X_test)}\")\n"
      ],
      "metadata": {
        "id": "UP-eCFQBlk-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad sequences for LSTM input\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(df['Cleaned_Title'])\n",
        "X_seq = tokenizer.texts_to_sequences(df['Cleaned_Title'])\n",
        "X_padded = pad_sequences(X_seq, maxlen=100)\n",
        "\n",
        "# Split padded data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define maximum number of words and sequence length\n",
        "MAX_VOCAB_SIZE = 5000  # Number of words in the vocabulary\n",
        "MAX_SEQUENCE_LENGTH = 100  # Length of each input sequence\n",
        "EMBEDDING_DIM = 128  # Embedding output size\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#print(model.summary())\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "EY7Dtxuul1ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot accuracy and loss curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Tc72x_xOl6YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model as an .h5 file\n",
        "model.save('sentiment_model.h5')\n",
        "print(\"Model saved successfully as 'sentiment_model.h5'.\")"
      ],
      "metadata": {
        "id": "ItjzJdA4R5vQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}